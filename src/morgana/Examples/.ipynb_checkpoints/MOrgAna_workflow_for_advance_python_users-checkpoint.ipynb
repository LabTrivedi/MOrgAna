{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"80\" height=\"80\" img src=\"app_screenshots/morgana_icon.png\" alt=\"morgana\">\n",
    "\n",
    "\n",
    "# MOrgAna workflow for advance python users  \n",
    "<br  />\n",
    "  \n",
    "This workflow is intended for users with programming background to analyse multiple image folders at once. Users can also use this notebook to select and adapt modules/functions specific to their purpose. This workflow follows the order of scripts in python_example_scripts and explains the code shown in the scripts in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Generate Masks\n",
    "This section makes use of the code from the following scripts:\n",
    "* '01_create_model_folder.py'\n",
    "* '02_create_ground_truth.py'\n",
    "* '03_train_networks.py'\n",
    "* '04_predict_masks.py'\n",
    "* '05_select_final_mask_method.py'\n",
    "* '05_select_final_mask_method.py'\n",
    "* '06_compute_final_masks_and_morphology.py'\n",
    "\n",
    "### 01_create_model_folder.py\n",
    "The following code chooses images from the acquired dataset to form the training dataset for the generation of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "\n",
    "print('Image subfolders found in: ' + parent_folder)\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select images for training dataset\n",
    "start = 0 # increase value to exclude starting images in dataset\n",
    "dN = 0 # every dNth image will be used for the training dataset; if dN = 0, random images are taken\n",
    "\n",
    "# True: create one model for all folders; False: create one model for each image subfolder\n",
    "combine_subfolders = True\n",
    "   \n",
    "# add folders that you want to ignore here\n",
    "exclude_folder = ['model_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_folder(folder, dN=30, start=0, combine=True):\n",
    "    \n",
    "    # create folders\n",
    "    if combine:\n",
    "        model_folder = os.path.join(os.path.split(folder)[0],'model_')\n",
    "    else:\n",
    "        model_folder = os.path.join(os.path.split(folder)[0], 'model_' + os.path.split(folder)[1])\n",
    "\n",
    "    if not os.path.exists(model_folder):\n",
    "        os.mkdir(model_folder)\n",
    "        \n",
    "    trainingset_folder = os.path.join(model_folder,'trainingset')\n",
    "    if not os.path.exists(trainingset_folder):\n",
    "        os.mkdir(trainingset_folder)\n",
    "\n",
    "    # count images and extract trainingset file names\n",
    "    flist = glob.glob(os.path.join(folder,'*.tif'))\n",
    "    flist.sort()\n",
    "    if dN:\n",
    "        flist = flist[start::dN]\n",
    "    else: \n",
    "        rng = default_rng()\n",
    "        random_choice = rng.choice(len(flist), size=np.clip(len(flist)//10, 1, None), replace=False)\n",
    "        flist = [flist[i] for i in random_choice]\n",
    "    \n",
    "    # copy images to trainingset folder\n",
    "    for f in flist:\n",
    "        fname = os.path.split(folder)[1] + '_' + os.path.split(f)[-1]\n",
    "        newf = os.path.join(trainingset_folder,fname)\n",
    "        if not os.path.exists(newf):\n",
    "            shutil.copy(f,newf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute parent folder as absolute path\n",
    "parent_folder = os.path.abspath(parent_folder)\n",
    "    \n",
    "# find out all image subfolders in parent_folder\n",
    "folder_names = next(os.walk(parent_folder))[1] \n",
    "    \n",
    "# exclude folders in exclude_folder\n",
    "folder_names = [g for g in folder_names if not g in exclude_folder ]\n",
    "\n",
    "for folder_name in tqdm(folder_names):\n",
    "    if not folder_name in exclude_folder:\n",
    "        folder_path = os.path.join(parent_folder, folder_name)\n",
    "\n",
    "        # for the parent_folder/every image subfolder, generate model folder and the trainingset\n",
    "        initialize_model_folder(folder_path, dN=dN, start=start, combine=combine_subfolders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02_create_ground_truth.py\n",
    "This script creates binary masks (ground truths) for images copied into the trainingset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell if starting from 02_create_ground_truth.py\n",
    "import os, glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "\n",
    "print('Image subfolders found in: ' + parent_folder)\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morgana.GUIs.manualmask import makeManualMask\n",
    "from morgana.DatasetTools import io\n",
    "import PyQt5.QtWidgets\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GT_mask(model_folder, app):\n",
    "    \n",
    "    ### check that model and trainingset exist\n",
    "    if not os.path.exists(model_folder):\n",
    "        print('Warning!')\n",
    "        print(model_folder,':')\n",
    "        print('Model folder not created! Skipping this subfolder.')\n",
    "        return\n",
    "        \n",
    "    trainingset_folder = os.path.join(model_folder,'trainingset')\n",
    "    if not os.path.exists(trainingset_folder):\n",
    "        print('Warning!')\n",
    "        print(model_folder,':')\n",
    "        print('Trainingset images not found! Skipping this subfolder.')\n",
    "        return\n",
    "\n",
    "    ### load trainingset images and previously generated ground truth    \n",
    "    flist_in = io.get_image_list(trainingset_folder, string_filter='_GT', mode_filter='exclude')\n",
    "    flist_in.sort()\n",
    "    flist_gt = io.get_image_list(trainingset_folder, string_filter='_GT', mode_filter='include')\n",
    "    flist_gt.sort()\n",
    "\n",
    "    ### if no trainingset images in the folder, skip this gastruloid\n",
    "    if len(flist_in) == 0:\n",
    "        print('\\n\\nWarning, no trainingset!','Selected \"'+model_folder+'\" but no trainingset *data* detected. Transfer some images in the \"trainingset\" folder.')\n",
    "        return\n",
    "    \n",
    "    ### if there are more trainingset than ground truth, promptuse to make mask\n",
    "    if len(flist_in)!=len(flist_gt):\n",
    "        print('\\n\\nWarning, trainingset incomplete!','Selected \"'+model_folder+'\" but not all masks have been created.\\nPlease provide manually annotated masks.')\n",
    "\n",
    "        for f in flist_in:\n",
    "            fn,ext = os.path.splitext(f)\n",
    "            mask_name = fn+'_GT'+ext\n",
    "            \n",
    "            if not os.path.exists(mask_name):\n",
    "                if not PyQt5.QtWidgets.QApplication.instance():\n",
    "                    app = PyQt5.QtWidgets.QApplication(sys.argv)\n",
    "                else:\n",
    "                    app = PyQt5.QtWidgets.QApplication.instance() \n",
    "                m = makeManualMask(f,subfolder='',fn=fn+'_GT'+ext,wsize = (2000,2000))\n",
    "                m.show()\n",
    "                app.exec_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folders = glob.glob(os.path.join(parent_folder,'model_*'))\n",
    "\n",
    "### compute parent folder as absolute path\n",
    "model_folders = [os.path.abspath(i) for i in model_folders]\n",
    "\n",
    "app = PyQt5.QtWidgets.QApplication(sys.argv)\n",
    "for model_folder in tqdm(model_folders):\n",
    "    create_GT_mask(model_folder, app)\n",
    "app.quit()\n",
    "print('All binary masks/ground truth images found. Move to the next step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03_train_networks.py\n",
    "This trains the model for further image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell if starting from 03_train_networks.py\n",
    "import os, glob\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "\n",
    "print('Image subfolders found in: ' + parent_folder)\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists\n",
    "\n",
    "model_folders = glob.glob(os.path.join(parent_folder,'model_*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "import time\n",
    "from morgana.DatasetTools import io as ioDT\n",
    "from morgana.MLModel import io as ioML\n",
    "from morgana.MLModel import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define parameters for feature generation for network training\n",
    "sigmas = [1.0, 5.0, 15.0]\n",
    "downscaling = 1.\n",
    "edge_size = 2\n",
    "pxl_extract_fraction = 0.25\n",
    "pxl_extract_bias = 0.4\n",
    "feature_type = 'ilastik' # 'daisy' or 'ilastik'\n",
    "deep = False # True: deep learning with Multi Layer Perceptrons; False: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute parent folder as absolute path\n",
    "model_folders = [os.path.abspath(i) for i in model_folders]\n",
    "\n",
    "for model_folder in model_folders:\n",
    "    print('-------------'+model_folder+'------------')\n",
    "\n",
    "    training_folder = os.path.join(model_folder, 'trainingset')\n",
    "\n",
    "    ### load images\n",
    "    flist_in = ioDT.get_image_list(\n",
    "                                              training_folder, \n",
    "                                              string_filter='_GT', \n",
    "                                              mode_filter='exclude'\n",
    "                                              )\n",
    "    img_train = []\n",
    "    for f in flist_in:\n",
    "        img = imread(f)\n",
    "        if len(img.shape)==2:\n",
    "            img = np.expand_dims(img,0)\n",
    "        if img.shape[-1] == np.min(img.shape):\n",
    "            img = np.moveaxis(img, -1, 0)\n",
    "        img_train.append( img[0] )\n",
    "\n",
    "    ## load ground truth\n",
    "    flist_gt = ioDT.get_image_list(\n",
    "                                            training_folder, \n",
    "                                            string_filter='_GT', \n",
    "                                            mode_filter='include'\n",
    "                                            )\n",
    "    gt_train = [ imread(f) for f in flist_gt ]\n",
    "    gt_train = [ g.astype(int) for g in gt_train ]\n",
    "\n",
    "    print('##### Training set:')\n",
    "    for i,f in enumerate(zip(flist_in,flist_gt)):\n",
    "        print(i+1,'\\t', os.path.split(f[0])[-1],'\\t', os.path.split(f[1])[-1])\n",
    "\n",
    "    ###################################################################\n",
    "    ### compute features and generate training set and weights\n",
    "\n",
    "    print('##### Generating training set...')\n",
    "    X, Y, w, scaler = train.generate_training_set( \n",
    "                                    img_train, \n",
    "                                    [g.astype(np.uint8) for g in gt_train], \n",
    "                                    sigmas = sigmas,\n",
    "                                    down_shape = downscaling,\n",
    "                                    edge_size = edge_size,\n",
    "                                    fraction = pxl_extract_fraction,\n",
    "                                    feature_mode = feature_type,\n",
    "                                    bias = pxl_extract_bias \n",
    "                                    )\n",
    "print(\"Features extracted. Move to the next step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Train the model (with Multi Layer Perceptrons, please ensure you have cuDNN installed)\n",
    "for model_folder in model_folders:\n",
    "    print('##### Training model...')\n",
    "    start = time.time()\n",
    "    classifier = train.train_classifier( X, Y, w, deep = deep )\n",
    "    print('Models trained in %.3f seconds.'%(time.time()-start))\n",
    "    if not deep:\n",
    "        print('classes_: ', classifier.classes_)\n",
    "        print('coef_: ', classifier.coef_)\n",
    "    \n",
    "    ### Save the model\n",
    "    ioML.save_model( \n",
    "                model_folder,\n",
    "                classifier,\n",
    "                scaler,\n",
    "                sigmas = sigmas,\n",
    "                down_shape = downscaling,\n",
    "                edge_size = edge_size,\n",
    "                fraction = pxl_extract_fraction,\n",
    "                feature_mode = feature_type,\n",
    "                bias = pxl_extract_bias,\n",
    "                deep = deep\n",
    "                )\n",
    "    print('##### Model saved!')\n",
    "\n",
    "print('All models saved, move to the next step.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04_predict_masks.py\n",
    "Generate binary masks for image dataset using previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell if starting from 04_train_networks.py\n",
    "import os, glob\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "\n",
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "\n",
    "print('Image subfolders found in: ' + parent_folder)\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists\n",
    "\n",
    "# add folders that you want to ignore here\n",
    "exclude_folder = []\n",
    "deep = False # True: deep learning with Multi Layer Perceptrons; False: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imsave\n",
    "import scipy.ndimage as ndi\n",
    "import multiprocessing\n",
    "from itertools import repeat\n",
    "from morgana.DatasetTools import io as ioDT\n",
    "import morgana.DatasetTools.multiprocessing.istarmap\n",
    "from morgana.MLModel import io as ioML\n",
    "from morgana.MLModel import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out all image subfolders in parent_folder\n",
    "folder_names = next(os.walk(parent_folder))[1] \n",
    "\n",
    "model_folders = glob.glob(os.path.join(parent_folder,'model_*'))\n",
    "model_folders_name = [os.path.split(model_folder)[-1] for model_folder in model_folders]\n",
    "\n",
    "# exclude folders in exclude_folder\n",
    "exclude_folder = ['']\n",
    "\n",
    "image_folders = [g for g in folder_names if not g in model_folders_name + exclude_folder]\n",
    "image_folders = [os.path.join(parent_folder, i) for i in image_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(image_folders)):\n",
    "    \n",
    "    image_folder = image_folders[i]\n",
    "    if len(model_folders)>1:\n",
    "        model_folder = model_folders[i]\n",
    "    else:\n",
    "        model_folder = model_folders[0]\n",
    "\n",
    "    print('-------------'+image_folder+'------------')\n",
    "    print('##### Loading classifier model and parameters...')\n",
    "    classifier, scaler, params = ioML.load_model( model_folder, deep = deep)\n",
    "    print('##### Model loaded!')\n",
    "\n",
    "    #######################################################################\n",
    "    ### apply classifiers and save images\n",
    "\n",
    "    result_folder = os.path.join(image_folder, 'result_segmentation')\n",
    "    if not os.path.exists(result_folder):\n",
    "        os.mkdir(result_folder)\n",
    "\n",
    "    flist_in = ioDT.get_image_list(image_folder)\n",
    "    flist_in.sort()        \n",
    "    N_img = len(flist_in)\n",
    "\n",
    "    # multiprocess\n",
    "    N_cores = np.clip( int(0.8 * multiprocessing.cpu_count()),1,None )\n",
    "\n",
    "    # try using multiprocessing (only for LR classifier)\n",
    "    if not deep:\n",
    "        pool = multiprocessing.Pool(N_cores)\n",
    "        _ = list(   tqdm(\n",
    "                                pool.istarmap(\n",
    "                                    predict.predict_image_from_file, \n",
    "                                    zip(    flist_in, \n",
    "                                            repeat(classifier),\n",
    "                                            repeat(scaler),\n",
    "                                            repeat(params),\n",
    "                                            repeat(deep) ) ), \n",
    "                                    total = N_img ) )\n",
    "    else:\n",
    "        for i in tqdm(range(N_img)):\n",
    "            predict.predict_image_from_file(flist_in[i], classifier, scaler, params, deep=deep)\n",
    "\n",
    "print('All images done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05_select_final_mask_method.py\n",
    "Visual inspection of masks generated by model and selection of final mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell if starting from 05_select_final_mask_method.py\n",
    "import os, glob, sys\n",
    "import PyQt5.QtWidgets\n",
    "\n",
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "\n",
    "print('Image subfolders found in: ' + parent_folder)\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists\n",
    "\n",
    "# find out all image subfolders in parent_folder\n",
    "folder_names = next(os.walk(parent_folder))[1] \n",
    "\n",
    "model_folders = glob.glob(os.path.join(parent_folder,'model_*'))\n",
    "model_folders_name = [os.path.split(model_folder)[-1] for model_folder in model_folders]\n",
    "\n",
    "# exclude folders in exclude_folder\n",
    "exclude_folder = ['']\n",
    "\n",
    "image_folders = [g for g in folder_names if not g in model_folders_name + exclude_folder]\n",
    "image_folders = [os.path.join(parent_folder, i) for i in image_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morgana.GUIs import inspection\n",
    "from morgana.DatasetTools.segmentation import io as ioSeg\n",
    "from morgana.DatasetTools import io as ioDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "app = PyQt5.QtWidgets.QApplication(sys.argv)\n",
    "for image_folder in image_folders:\n",
    "\n",
    "    ### compute parent folder as absolute path\n",
    "    image_folder = os.path.abspath(image_folder)\n",
    "\n",
    "    print('\\n-------------'+image_folder+'------------\\n')\n",
    "\n",
    "    flist_in = ioDT.get_image_list(image_folder)\n",
    "    n_imgs = len( flist_in )\n",
    "    if os.path.exists(os.path.join(image_folder,'result_segmentation','segmentation_params.csv')):\n",
    "        flist_in, chosen_masks, down_shapes, thinnings, smoothings = ioSeg.load_segmentation_params( os.path.join(image_folder,'result_segmentation') )\n",
    "        flist_in = [os.path.join(image_folder,i) for i in flist_in]\n",
    "    else:\n",
    "        chosen_masks = ['w' for i in range(n_imgs)]\n",
    "        down_shapes = [0.50 for i in range(n_imgs)]\n",
    "        thinnings = [10 for i in range(n_imgs)]\n",
    "        smoothings = [25 for i in range(n_imgs)]\n",
    "\n",
    "    save_folder = os.path.join(image_folder, 'result_segmentation')\n",
    "    ioSeg.save_segmentation_params(  save_folder, \n",
    "                                                    [os.path.split(fin)[-1] for fin in flist_in],\n",
    "                                                    chosen_masks,\n",
    "                                                    down_shapes, \n",
    "                                                    thinnings, \n",
    "                                                    smoothings )\n",
    "\n",
    "    if not os.path.exists(save_folder):\n",
    "        if not PyQt5.QtWidgets.QApplication.instance():\n",
    "            app = PyQt5.QtWidgets.QApplication(sys.argv)\n",
    "        else:\n",
    "            app = PyQt5.QtWidgets.QApplication.instance() \n",
    "    w = inspection.inspectionWindow_20max(\n",
    "            image_folder, \n",
    "            parent=None, \n",
    "            start=0, \n",
    "            stop=20\n",
    "            )\n",
    "    w.show()\n",
    "    app.exec()\n",
    "app.quit()\n",
    "print('All final masks selected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06_compute_final_masks_and_morphology.py\n",
    "The following code computes the final masks used for quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell if starting from 06_compute_final_masks_and_morphology.py\n",
    "import os, glob, sys\n",
    "from skimage.io import imread, imsave\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists\n",
    "\n",
    "# find out all image subfolders in parent_folder\n",
    "folder_names = next(os.walk(parent_folder))[1] \n",
    "\n",
    "model_folders = glob.glob(os.path.join(parent_folder,'model_*'))\n",
    "model_folders_name = [os.path.split(model_folder)[-1] for model_folder in model_folders]\n",
    "\n",
    "# exclude folders in exclude_folder\n",
    "exclude_folder = ['']\n",
    "\n",
    "image_folders = [g for g in folder_names if not g in model_folders_name + exclude_folder]\n",
    "image_folders = [os.path.join(parent_folder, i) for i in image_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morgana.DatasetTools import io as ioDT\n",
    "from morgana.DatasetTools.segmentation import io as ioSeg\n",
    "from morgana.DatasetTools.morphology import io as ioMorph\n",
    "from morgana.DatasetTools.morphology import computemorphology, overview\n",
    "from morgana.ImageTools.segmentation import segment\n",
    "from morgana.GUIs import manualmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_folder in image_folders:\n",
    "\n",
    "    ### compute parent folder as absolute path\n",
    "    image_folder = os.path.abspath(image_folder)\n",
    "\n",
    "    print('-------------'+image_folder+'------------')\n",
    "\n",
    "    result_folder = os.path.join(image_folder, 'result_segmentation')\n",
    "    folder, cond = os.path.split(image_folder)\n",
    "\n",
    "    flist_in, chosen_masks, down_shapes, thinnings, smoothings = ioSeg.load_segmentation_params( result_folder )\n",
    "    flist_in = [os.path.join(image_folder, f) for f in flist_in]\n",
    "    n_imgs = len(flist_in)\n",
    "\n",
    "    #######################################################################\n",
    "    ### clean masks previously generated\n",
    "    ### only use this part if you want to rewrite final masks!\n",
    "\n",
    "#        flist_to_remove = ioDT.get_image_list(result_folder, '_finalMask', 'include')\n",
    "#        for f in flist_to_remove:\n",
    "#            os.remove(f)\n",
    "#        morpho_file = os.path.join(result_folder,cond+'_morpho_params.json')\n",
    "#        if os.path.exists(morpho_file):\n",
    "#            os.remove(morpho_file)\n",
    "\n",
    "    #######################################################################\n",
    "    ### generate final mask if not there yet\n",
    "    print('Generating smooth masks:')\n",
    "\n",
    "    for i in tqdm(range(n_imgs)):\n",
    "\n",
    "        folder, filename = os.path.split(flist_in[i])\n",
    "        filename, extension = os.path.splitext(filename)\n",
    "\n",
    "        final_mask_name = os.path.join(result_folder,filename+'_finalMask'+extension)\n",
    "        if not os.path.exists(final_mask_name):\n",
    "\n",
    "            if chosen_masks[i] == 'w':\n",
    "                _rawmask = imread( os.path.join(result_folder, filename+'_watershed'+extension) )\n",
    "                mask = segment.smooth_mask( \n",
    "                                            _rawmask, \n",
    "                                            mode='watershed',\n",
    "                                            down_shape=down_shapes[i], \n",
    "                                            smooth_order=smoothings[i] \n",
    "                                            )\n",
    "                while (np.sum(mask)==0)&(smoothings[i]>5):\n",
    "                    print('Mask failed...')\n",
    "                    # if mask is zero, try smoothing less\n",
    "                    smoothings[i] -= 2\n",
    "                    print('Trying with: smoothing', smoothings[i])\n",
    "                    mask = segment.smooth_mask( \n",
    "                                                _rawmask, \n",
    "                                                mode='watershed',\n",
    "                                                down_shape=down_shapes[i], \n",
    "                                                smooth_order=smoothings[i] \n",
    "                                                )\n",
    "\n",
    "            elif chosen_masks[i] == 'c':\n",
    "                _rawmask = imread( os.path.join(result_folder, filename+'_classifier'+extension) )\n",
    "                mask = segment.smooth_mask( \n",
    "                                            _rawmask, \n",
    "                                            mode='classifier',\n",
    "                                            down_shape=down_shapes[i], \n",
    "                                            smooth_order=smoothings[i],\n",
    "                                            thin_order=thinnings[i] \n",
    "                                            )\n",
    "                while (np.sum(mask)==0)&(smoothings[i]>5)&(thinnings[i]>1):\n",
    "                    print('Mask failed...')\n",
    "                    # if mask is zero, try smoothing less\n",
    "                    smoothings[i] -= 2\n",
    "                    thinnings[i] -= 1\n",
    "                    print('Trying with: smoothing', smoothings[i],' thinnings', thinnings[i])\n",
    "                    mask = segment.smooth_mask( \n",
    "                                                _rawmask, \n",
    "                                                mode='classifier',\n",
    "                                                down_shape=down_shapes[i], \n",
    "                                                smooth_order=smoothings[i],\n",
    "                                                thin_order=thinnings[i] \n",
    "                                                )\n",
    "\n",
    "            elif chosen_masks[i] == 'm':\n",
    "                if not os.path.exists(os.path.join(result_folder,filename+'_manual'+extension)):\n",
    "                    m = manualmask.makeManualMask(flist_in[i])\n",
    "                    m.show()\n",
    "                    m.exec()\n",
    "                else:\n",
    "                    print('A previously generated manual mask exists!')\n",
    "                _rawmask = imread( os.path.join(result_folder,filename+'_manual'+extension) )\n",
    "                mask = segment.smooth_mask( \n",
    "                                                _rawmask, \n",
    "                                                mode='manual',\n",
    "                                                down_shape=down_shapes[i], \n",
    "                                                smooth_order=smoothings[i] \n",
    "                                                )\n",
    "                while (np.sum(mask)==0)&(smoothings[i]>5):\n",
    "                    print('Mask failed...')\n",
    "                    # if mask is zero, try smoothing less\n",
    "                    smoothings[i] -= 2\n",
    "                    print('Trying with: smoothing', smoothings[i])\n",
    "                    # if mask is zero, try smoothing less\n",
    "                    smoothings[i] -= 2\n",
    "                    mask = segment.smooth_mask( \n",
    "                                                    _rawmask, \n",
    "                                                    mode='manual',\n",
    "                                                    down_shape=down_shapes[i], \n",
    "                                                    smooth_order=smoothings[i] \n",
    "                                                    )\n",
    "            elif chosen_masks[i] == 'i': \n",
    "                continue\n",
    "\n",
    "            if np.sum(mask) == 0:\n",
    "                print('Warning, no trainingset!','The method selected didn\\'t generate a valid mask. Please input the mask manually.')\n",
    "\n",
    "                chosen_masks[i] = 'm'\n",
    "                ioSeg.save_segmentation_params(  \n",
    "                                    result_folder, \n",
    "                                    [os.path.split(fin)[-1] for fin in flist_in],\n",
    "                                    chosen_masks,\n",
    "                                    down_shapes, \n",
    "                                    thinnings, \n",
    "                                    smoothings \n",
    "                                    )\n",
    "                if not os.path.exists(os.path.join(result_folder,filename+'_manual'+extension)):\n",
    "                    m = manualmask.makeManualMask(flist_in[i])\n",
    "                    m.show()\n",
    "                    m.exec()\n",
    "                else:\n",
    "                    print('A previously generated manual mask exists!')\n",
    "                _rawmask = imread( os.path.join(result_folder,filename+'_manual'+extension) )\n",
    "                mask = segment.smooth_mask( \n",
    "                                _rawmask, \n",
    "                                mode='manual',\n",
    "                                down_shape=down_shapes[i], \n",
    "                                smooth_order=smoothings[i] \n",
    "                                )\n",
    "\n",
    "            ### save segmentation parameters\n",
    "            ioSeg.save_segmentation_params(  \n",
    "                            result_folder, \n",
    "                            [os.path.split(fin)[-1] for fin in flist_in],\n",
    "                            chosen_masks,\n",
    "                            down_shapes, \n",
    "                            thinnings, \n",
    "                            smoothings \n",
    "                            )\n",
    "\n",
    "\n",
    "            ### save final mask\n",
    "            imsave(final_mask_name, mask)\n",
    "\n",
    "print('### Done computing masks! Images ready for quantification.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Quantification\n",
    "This section makes use of the code from the following scripts:\n",
    "* '07_compute_straighten_morpho_and_fluo.py'\n",
    "* '08_make_overview_plots.py'\n",
    "* '09_make_morphology_plots.py'\n",
    "* '10_make_fluorescence_plots.py'\n",
    "\n",
    "Other plot options:\n",
    "* '11_bra_pole_vs_morpho_midline.py'\n",
    "\n",
    "### 07_compute_straighten_morpho_and_fluo.py\n",
    "The following code computes morphological parameters based on the binary masks and fluorescence information based on both binary masks and fluorescence channel images. Resulting values are saved and found in 'folder_name_morpho_params.json', 'folder_name_morpho_straight_params.json', 'folder_name_fluo_intensity.json' in the 'result_segmentation' folder in image subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell if starting from  07_compute_straighten_morpho_and_fluo.py\n",
    "import os, glob\n",
    "from skimage.io import imread, imsave\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "print('Image subfolders found in: ' + parent_folder)\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists\n",
    "\n",
    "# find out all image subfolders in parent_folder\n",
    "folder_names = next(os.walk(parent_folder))[1] \n",
    "\n",
    "model_folders = glob.glob(os.path.join(parent_folder,'model_*'))\n",
    "model_folders_name = [os.path.split(model_folder)[-1] for model_folder in model_folders]\n",
    "\n",
    "# exclude folders in exclude_folder\n",
    "exclude_folder = ['']\n",
    "\n",
    "image_folders = [g for g in folder_names if not g in model_folders_name + exclude_folder]\n",
    "image_folders = [os.path.join(parent_folder, i) for i in image_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morgana.DatasetTools.morphology import overview\n",
    "from morgana.DatasetTools.morphology import computemorphology\n",
    "from morgana.DatasetTools.morphology import io as ioMorph\n",
    "from morgana.DatasetTools.straightmorphology import computestraightmorphology\n",
    "from morgana.DatasetTools.straightmorphology import io as ioStr\n",
    "from morgana.DatasetTools.fluorescence import computefluorescence\n",
    "from morgana.DatasetTools.fluorescence import io as ioFluo\n",
    "from morgana.GUIs import inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_folder in image_folders:\n",
    "\n",
    "    ### compute parent folder as absolute path\n",
    "    image_folder = os.path.abspath(image_folder)\n",
    "\n",
    "    print('-------------'+image_folder+'------------')\n",
    "\n",
    "    result_folder = os.path.join(image_folder, 'result_segmentation')\n",
    "    folder, cond = os.path.split(image_folder)\n",
    "\n",
    "    #######################################################################  \n",
    "    ### compute composite and meshgrid overview\n",
    "\n",
    "    file = '_composite_recap.tif'\n",
    "    text = 'Composite files saved at:'\n",
    "    parent,cond = os.path.split(image_folder)\n",
    "    fname = os.path.join(image_folder,'result_segmentation', cond + file)\n",
    "    text = text + '\\n\\t'+fname\n",
    "    if not os.path.exists(os.path.join(result_folder,image_folder+file)):\n",
    "        overview.createCompositeOverview(image_folder, keep_open=False)        \n",
    "    print(text)\n",
    "\n",
    "    file = '_meshgrid_recap.png'\n",
    "    text = 'Meshgrid files saved at:'\n",
    "    parent,cond = os.path.split(image_folder)\n",
    "    fname = os.path.join(image_folder,'result_segmentation', cond + file)\n",
    "    text = text + '\\n\\t'+fname\n",
    "    if not os.path.exists(os.path.join(result_folder,image_folder+file)):\n",
    "        overview.createMeshgridOverview(image_folder, keep_open=False)\n",
    "    print(text)\n",
    "\n",
    "    #######################################################################\n",
    "    ### compute morphology if not computed\n",
    "    compute_morphological_info = computemorphology.compute_morphological_info\n",
    "    save_morphological_info = ioMorph.save_morpho_params\n",
    "\n",
    "    file_extension = '_morpho_params.json'\n",
    "    fname = os.path.join(result_folder,cond+file_extension)\n",
    "\n",
    "    if not os.path.exists(fname):\n",
    "        data = compute_morphological_info(image_folder, False)\n",
    "        save_morphological_info(result_folder, cond, data)\n",
    "\n",
    "    #######################################################################\n",
    "    ### compute straight morphology if not computed\n",
    "\n",
    "    compute_morphological_info = computestraightmorphology.compute_straight_morphological_info\n",
    "    save_morphological_info = ioStr.save_straight_morpho_params\n",
    "\n",
    "    file_extension = '_morpho_straight_params.json'\n",
    "    fname = os.path.join(result_folder,cond+file_extension)\n",
    "\n",
    "    if not os.path.exists(fname):\n",
    "        data = compute_morphological_info( image_folder )\n",
    "        save_morphological_info( result_folder, cond, data )\n",
    "\n",
    "    print(\"Computed all straight morphology information\")\n",
    "\n",
    "    #######################################################################  \n",
    "    ### compute fluorescence info if not computed\n",
    "\n",
    "    file_extension = '_fluo_intensity.json'\n",
    "    fname = os.path.join(result_folder,cond+file_extension)\n",
    "\n",
    "    if not os.path.exists(fname):\n",
    "        data = computefluorescence.compute_fluorescence_info( image_folder )\n",
    "        ioFluo.save_fluo_info( result_folder, cond, data )\n",
    "\n",
    "    print(\"Computed all fluorescence information\")\n",
    "\n",
    "    #######################################################################        \n",
    "    ### clean up watershed and classifier masks\n",
    "\n",
    "    flist = glob.glob(os.path.join(result_folder,'*_watershed.tif'))            \n",
    "    print('Cleaning up watershed masks')\n",
    "    for f in tqdm(flist):\n",
    "        os.remove(f)\n",
    "\n",
    "    flist = glob.glob(os.path.join(result_folder,'*_classifier.tif'))            \n",
    "    print('Cleaning up classifier masks')\n",
    "    for f in tqdm(flist):\n",
    "        os.remove(f)\n",
    "\n",
    "    flist = glob.glob(os.path.join(result_folder,'*_manual.tif'))            \n",
    "    print('Cleaning up manual masks')\n",
    "    for f in tqdm(flist):\n",
    "        os.remove(f)\n",
    "print('Done! All morphology and fluorescence information computed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08_make_overview_plots.py; 09_make_morphology_plots.py; 10_make_fluorescence_plots.py\n",
    "\n",
    "This script creates overviews and quantitative plots based on morphology and fluorescence information previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell if starting from 08_make_plots.py\n",
    "import sys, time, tqdm, os, glob\n",
    "from skimage.io import imread, imsave\n",
    "import numpy as np\n",
    "import PyQt5.QtWidgets\n",
    "\n",
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "\n",
    "print('Image subfolders found in: ' + parent_folder)\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists\n",
    "\n",
    "# find out all image subfolders in parent_folder\n",
    "folder_names = next(os.walk(parent_folder))[1] \n",
    "\n",
    "model_folders = glob.glob(os.path.join(parent_folder,'model_*'))\n",
    "model_folders_name = [os.path.split(model_folder)[-1] for model_folder in model_folders]\n",
    "\n",
    "# exclude folders in exclude_folder\n",
    "exclude_folder = ['']\n",
    "\n",
    "image_folders = [g for g in folder_names if not g in model_folders_name + exclude_folder]\n",
    "image_folders = [os.path.join(parent_folder, i) for i in image_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from morgana.DatasetTools.morphology import overview as overviewDT\n",
    "from morgana.DatasetTools import arrangemorphodata\n",
    "from morgana.DatasetTools import arrangefluodata\n",
    "from collections.abc import Iterable\n",
    "from morgana.GUIs import visualize0d\n",
    "from morgana.GUIs import visualize1d\n",
    "from morgana.GUIs import visualize2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Create Composite & Meshgrid Overviews for each image subfolder\n",
    "for image_folder in image_folders:\n",
    "    overviewDT.createCompositeOverview(image_folder)\n",
    "    overviewDT.createMeshgridOverview(image_folder)\n",
    "    parent,cond = os.path.split(image_folder)\n",
    "    text = 'Composite and Meshgrid files saved at:' + '\\n\\t'+ os.path.join(os.path.split(parent)[-1],'result_segmentation', cond)\n",
    "    print('Completed successfully. ' + text)\n",
    "print('All overviews created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morphology Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select morphological parameters to be quantified\n",
    "\n",
    "maskType = 'Straightened' # Use 'Unprocessed' or 'Straightened' binary mask\n",
    "Timelapse = False # Do images in the folder belong to a timelapse?\n",
    "\n",
    "all_morpho_params = False # select true if all parameters are to be used.\n",
    "# otherwise, select which paramters you would like to compute.\n",
    "area = False\n",
    "eccentricity = True\n",
    "major_axis_length = True\n",
    "minor_axis_length = True\n",
    "equivalent_diameter = False\n",
    "perimeter = False\n",
    "euler_number = False\n",
    "extent = False\n",
    "form_factor = False\n",
    "orientation = False\n",
    "locoefa_coeff = True\n",
    "\n",
    "# Define number of groups/number of conditions and the image subfolders that belong to each group\n",
    "group1 = [image_folders[0], image_folders[1]]\n",
    "group2 = [image_folders[2]]\n",
    "group3 = [image_folders[3]]\n",
    "\n",
    "groups = [group1, group2, group3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show plots\n",
    "%matplotlib qt\n",
    "\n",
    "morphoKeys = ['area',\n",
    "              'eccentricity',\n",
    "              'major_axis_length',\n",
    "              'minor_axis_length',\n",
    "              'equivalent_diameter',\n",
    "              'perimeter',\n",
    "              'euler_number',\n",
    "              'extent',\n",
    "              'form_factor',\n",
    "              'orientation',\n",
    "              'locoefa_coeff']\n",
    "\n",
    "if all_morpho_params:\n",
    "    computeMorpho = [True for key in morphoKeys]\n",
    "else:\n",
    "    computeMorpho = [area, eccentricity, major_axis_length, minor_axis_length, equivalent_diameter, \n",
    "                     perimeter, euler_number, extent, form_factor, orientation, locoefa_coeff]\n",
    "    \n",
    "# extract data from all the folders\n",
    "data_all, keys = arrangemorphodata.collect_morpho_data( groups, \n",
    "                                                        morphoKeys, \n",
    "                                                        computeMorpho, \n",
    "                                                        maskType, \n",
    "                                                        Timelapse\n",
    "                                                        )\n",
    "quantifier = []\n",
    "app = PyQt5.QtWidgets.QApplication(sys.argv)\n",
    "\n",
    "# for every quantification parameter, make the appropriate plot\n",
    "for key in keys:\n",
    "    data_key = [data[key] for data in data_all]\n",
    "\n",
    "    # find out number of dimensions of the data_key object by going deeper in the object\n",
    "    # and checking if the first item of layer n is iterable\n",
    "    iterable = True\n",
    "    ndim = 0\n",
    "    first_object = data_key[0][0]\n",
    "    while iterable:\n",
    "        iterable = isinstance(first_object, Iterable)\n",
    "        if iterable:\n",
    "            ndim += 1\n",
    "            first_object = first_object[0]\n",
    "    \n",
    "    if not PyQt5.QtWidgets.QApplication.instance():\n",
    "        app = PyQt5.QtWidgets.QApplication(sys.argv)\n",
    "    else:\n",
    "        app = PyQt5.QtWidgets.QApplication.instance() \n",
    "        \n",
    "    # call the right visualization tool according to the number of dimensions\n",
    "    ### clean up quantifier handler:\n",
    "    quantifier = [quantifier[i] for i in range(len(quantifier)) if quantifier[i] is not None]\n",
    "\n",
    "    if ndim == 0:\n",
    "        quantifier.append( visualize0d.visualization_0d( data_key, key ) )\n",
    "        quantifier[-1].show()\n",
    "    elif ndim == 1:\n",
    "        quantifier.append( visualize1d.visualization_1d( data_key, key ) )\n",
    "        quantifier[-1].show()\n",
    "    elif ndim == 2:\n",
    "        quantifier.append( visualize2d.visualization_2d( data_key, key ) )\n",
    "        quantifier[-1].show()\n",
    "    app.exec()\n",
    "app.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fluorescence Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select parameters for fluorescence quantification\n",
    "\n",
    "channel = 1 # number of fluorescence channel in order of channels in tif file.\n",
    "Timelapse = False # Do images in the folder belong to a timelapse?\n",
    "distribution = 'LRprofile' # choice of profile: 'Average','APprofile','LRprofile','RADprofile','ANGprofile'\n",
    "\n",
    "# Define number of groups/number of conditions and the image subfolders that belong to each group\n",
    "group1 = [image_folders[0], image_folders[1]]\n",
    "group2 = [image_folders[2]]\n",
    "group3 = [image_folders[3]]\n",
    "\n",
    "groups = [group1, group2, group3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show plots\n",
    "app = PyQt5.QtWidgets.QApplication(sys.argv)\n",
    "\n",
    "distributionType = ['Average','APprofile','LRprofile','RADprofile','ANGprofile']\n",
    "distributionType = distributionType[distributionType.index(distribution)]\n",
    "\n",
    "# extract data from all the folders\n",
    "data_all = arrangefluodata.collect_fluo_data(groups, \n",
    "                                             channel, \n",
    "                                             distributionType, \n",
    "                                             Timelapse)\n",
    "\n",
    "# if the result is None, something went wrong!\n",
    "if not data_all:\n",
    "    print('Warning, invalid channel!','The channel selected doesn\\'t appear in the raw data!')\n",
    "\n",
    "# make the appropriate plot\n",
    "data_key = [data['ch%d_%s'%(channel,distributionType)] for data in data_all]\n",
    "data_bckg = [data['ch%d_Background'%(channel)] for data in data_all]\n",
    "\n",
    "# find out number of dimensions of the data_key object by going deeper in the object\n",
    "# and checking if the first item of layer n is iterable\n",
    "iterable = True\n",
    "ndim = 0\n",
    "first_object = data_key[0][0]\n",
    "quantifier = []\n",
    "while iterable:\n",
    "    iterable = isinstance(first_object, Iterable)\n",
    "    if iterable:\n",
    "        ndim += 1\n",
    "        first_object = first_object[0]\n",
    "\n",
    "# call the right visualization tool according to the number of dimensions\n",
    "### clean up quantifier handler:\n",
    "quantifier = [quantifier[i] for i in range(len(quantifier)) if quantifier[i] is not None]\n",
    "if not PyQt5.QtWidgets.QApplication.instance():\n",
    "    app = PyQt5.QtWidgets.QApplication(sys.argv)\n",
    "else:\n",
    "    app = PyQt5.QtWidgets.QApplication.instance() \n",
    "if ndim == 0:\n",
    "    quantifier.append( visualize0d.visualization_0d( data_key, distributionType, background=data_bckg ) )\n",
    "    quantifier[-1].show()\n",
    "elif ndim == 1:\n",
    "    quantifier.append( visualize1d.visualization_1d( data_key, distributionType, background=data_bckg ) )\n",
    "    quantifier[-1].show()\n",
    "elif ndim == 2:\n",
    "    quantifier.append( visualize2d.visualization_2d( data_key, distributionType, background=data_bckg ) )\n",
    "    quantifier[-1].show()\n",
    "app.exec()\n",
    "app.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11_bra_pole_vs_morpho_midline.py\n",
    "This script calculates where the GFP (first fluorescent channel) pole is with respect to the pole in the straightened organoid and saves the outputted values in 'Bra_pole_info.json' as well as saves an overview image (angle of pole with straighted organoid) for each organoid in the 'result_segmentation folder' in image subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell if starting from 09_bra_pole_vs_morpho_midline.py\n",
    "import sys, time, tqdm, os, glob\n",
    "from skimage.io import imread, imsave\n",
    "import numpy as np\n",
    "\n",
    "# select folder containing all image folders to be analysed\n",
    "# parent_folder = os.path.join('test_data','2020-09-22_conditions')\n",
    "\n",
    "print('Image subfolders found in: ' + parent_folder)\n",
    "if os.path.exists(parent_folder):\n",
    "    print('Path exists! Proceed!')# check if the path exists\n",
    "\n",
    "# find out all image subfolders in parent_folder\n",
    "folder_names = next(os.walk(parent_folder))[1] \n",
    "\n",
    "model_folders = glob.glob(os.path.join(parent_folder,'model_*'))\n",
    "model_folders_name = [os.path.split(model_folder)[-1] for model_folder in model_folders]\n",
    "\n",
    "# exclude folders in exclude_folder\n",
    "exclude_folder = ['']\n",
    "\n",
    "image_folders = [g for g in folder_names if not g in model_folders_name + exclude_folder]\n",
    "image_folders = [os.path.join(parent_folder, i) for i in image_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "import pandas as pd\n",
    "from scipy.ndimage import map_coordinates\n",
    "from skimage.filters import gaussian\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib import rc\n",
    "rc('font', size=12)\n",
    "rc('font', family='Arial')\n",
    "# rc('font', serif='Times')\n",
    "rc('pdf', fonttype=42)\n",
    "# rc('text', usetex=True)\n",
    "\n",
    "from morgana.ImageTools.morphology import meshgrid\n",
    "from morgana.DatasetTools.straightmorphology import io as ioStr\n",
    "from morgana.DatasetTools.morphology import io as ioMorph\n",
    "from morgana.DatasetTools.fluorescence import io as ioFluo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------/Volumes/trivedi/Jia_Le_Lim/morgana_example_datasets/gastruloids_ipynb/condB/condB_72h------------\n",
      "5.145844691748207\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|        | 1/5 [00:01<00:04,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.755359314766752\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|      | 2/5 [00:01<00:02,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160.47317680357045\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|    | 3/5 [00:02<00:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194.8388871136327\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|  | 4/5 [00:03<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213.54697857062266\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:03<00:00,  1.35it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------/Volumes/trivedi/Jia_Le_Lim/morgana_example_datasets/gastruloids_ipynb/condB/condB_48h------------\n",
      "0.5528075296727498\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|        | 1/5 [00:00<00:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10546375920597355\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|      | 2/5 [00:01<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.259285204989714\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|    | 3/5 [00:01<00:01,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.513807272802618\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|  | 4/5 [00:02<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.551123516011597\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:02<00:00,  1.75it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------/Volumes/trivedi/Jia_Le_Lim/morgana_example_datasets/gastruloids_ipynb/condB/condB_96h------------\n",
      "1535.3790509441533\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|        | 1/5 [00:00<00:03,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348.1621460428007\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|      | 2/5 [00:01<00:02,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1931.5543031330737\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|    | 3/5 [00:02<00:01,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2062.97897887312\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|  | 4/5 [00:03<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1727.3589338479865\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:04<00:00,  1.13it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------/Volumes/trivedi/Jia_Le_Lim/morgana_example_datasets/gastruloids_ipynb/condB/condB_120h------------\n",
      "791.4169889234682\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|        | 1/5 [00:01<00:05,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579.1183456290122\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|      | 2/5 [00:02<00:04,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551.4735134207165\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|    | 3/5 [00:04<00:03,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.37483772315846\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|  | 4/5 [00:06<00:01,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622.7070527834405\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:08<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity computed. Images and Bra_pole_info.json saved in result_segmentation folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### compute parent folder as absolute path\n",
    "parent_folder = os.path.abspath(parent_folder)\n",
    "%matplotlib qt\n",
    "\n",
    "for image_folder in image_folders:\n",
    "    print('-------------'+image_folder+'------------')\n",
    "    image_folder = os.path.split(image_folder)[-1]\n",
    "    save_folder = os.path.join(parent_folder,image_folder,'result_segmentation')\n",
    "\n",
    "    bra_fname = os.path.join(save_folder,'Bra_pole_info.json')\n",
    "\n",
    "    if not os.path.exists(bra_fname):\n",
    "\n",
    "        df_morpho = ioMorph.load_morpho_params(save_folder, image_folder)\n",
    "        df_straight = ioStr.load_straight_morpho_params(save_folder, image_folder)\n",
    "        df_fluo = ioFluo.load_fluo_info( save_folder, image_folder)\n",
    "\n",
    "        N_img = len(df_morpho.input_file)\n",
    "\n",
    "        x_morphopole = np.array([0. for i in range(N_img)])\n",
    "        y_morphopole = np.array([0. for i in range(N_img)])\n",
    "        axratios = np.array([0. for i in range(N_img)])\n",
    "\n",
    "        x_fluomax = np.array([0. for i in range(N_img)])\n",
    "        y_fluomax = np.array([0. for i in range(N_img)])\n",
    "        alphas_fluomax = np.array([0. for i in range(N_img)])\n",
    "\n",
    "        x_fluocm = np.array([0. for i in range(N_img)])\n",
    "        y_fluocm = np.array([0. for i in range(N_img)])\n",
    "        alphas_fluocm = np.array([0. for i in range(N_img)])\n",
    "\n",
    "        times = np.array([0. for i in range(N_img)])\n",
    "\n",
    "        for i in tqdm.trange(N_img):\n",
    "\n",
    "            times[i] = i\n",
    "\n",
    "            # load images\n",
    "            f_in = df_morpho.input_file[i]\n",
    "            f_ma = df_morpho.mask_file[i]\n",
    "            _slice = df_morpho.slice[i]\n",
    "            image = imread(os.path.join(parent_folder,image_folder,f_in))\n",
    "            image = np.stack([ img[_slice].astype(np.float) for img in image ])\n",
    "            bckg = df_fluo.ch1_Background[i]\n",
    "            image[1] = image[1].astype(float)-bckg\n",
    "            image[1] = np.clip(image[1],0,None)\n",
    "            mask = imread(os.path.join(parent_folder,image_folder,f_ma))[_slice]\n",
    "\n",
    "            # compute the meshgrid\n",
    "            tangent = df_morpho.tangent[i]\n",
    "            midline = df_morpho.midline[i]\n",
    "            width = df_morpho.meshgrid_width[i]\n",
    "            mesh = df_morpho.meshgrid[i]\n",
    "            if mesh == None:\n",
    "                mesh = meshgrid.compute_meshgrid(\n",
    "                                                                            midline,\n",
    "                                                                            tangent,\n",
    "                                                                            width\n",
    "                                                                            )\n",
    "\n",
    "            # straighten the mask and the image\n",
    "            mesh_shape = mesh.shape\n",
    "            coords_flat = np.reshape( mesh, (mesh_shape[0]*mesh_shape[1],2) ).T\n",
    "\n",
    "            ma_straight = map_coordinates(mask,\n",
    "                                          coords_flat,\n",
    "                                          order=0,\n",
    "                                          mode='constant',\n",
    "                                          cval=0).T\n",
    "            ma_straight = np.reshape(ma_straight,(mesh_shape[0],mesh_shape[1]))\n",
    "\n",
    "            fl_straight = map_coordinates(image[1],\n",
    "                          coords_flat,\n",
    "                          order=0,\n",
    "                          mode='constant',\n",
    "                          cval=0).T\n",
    "            fl_straight = np.reshape(fl_straight,(mesh_shape[0],mesh_shape[1]))\n",
    "            fl_straight_masked = fl_straight * ma_straight\n",
    "\n",
    "            bf_straight = map_coordinates(image[0],\n",
    "                          coords_flat,\n",
    "                          order=0,\n",
    "                          mode='constant',\n",
    "                          cval=0).T\n",
    "            bf_straight = np.reshape(bf_straight,(mesh_shape[0],mesh_shape[1]))\n",
    "\n",
    "            ( length, width ) = ma_straight.shape\n",
    "\n",
    "            # flip images if meshgrid wrong\n",
    "            APprof = df_fluo.ch1_APprofile[i]\n",
    "            first_half = APprof[:int(length/2)]\n",
    "            second_half = APprof[int(length/2):]\n",
    "            flip = False\n",
    "            if np.sum(first_half) < np.sum(second_half):\n",
    "                ma_straight = ma_straight[::-1]\n",
    "                fl_straight_masked = fl_straight_masked[::-1]\n",
    "                fl_straight = fl_straight[::-1]\n",
    "                bf_straight = bf_straight[::-1]\n",
    "                flip = True\n",
    "                \n",
    "            ##################################################################\n",
    "\n",
    "            # find coordinate of highest fluorescence value\n",
    "            fl_gauss = gaussian(fl_straight_masked, sigma=10)\n",
    "            max_val = np.max(fl_gauss[:int(length/2)])\n",
    "            max_pos = np.where(fl_gauss[:int(length/2)]==max_val)\n",
    "            print(max_val)\n",
    "            print(fl_gauss)\n",
    "            max_pos = np.array([i[0] for i in max_pos])\n",
    "\n",
    "            # find coordinate of highest fluorescence value in the upward\n",
    "            prop = regionprops(ma_straight, fl_straight_masked)\n",
    "            centroid_fluo = prop[0].weighted_centroid\n",
    "\n",
    "            # find coordinate of upward pole\n",
    "            pole_pos = np.array([0,int(width/2)])\n",
    "\n",
    "            # plot relevant variables\n",
    "            centroid = np.array(df_straight.centroid[i]).astype(np.uint16)\n",
    "            if flip:\n",
    "                centroid[0] = length - centroid[0]\n",
    "\n",
    "            fig1, ax1 = plt.subplots(1, 2, figsize=(10,5))\n",
    "            ax1[0].imshow(fl_straight,cmap='gray')\n",
    "            ax1[0].plot([centroid[1],pole_pos[1]],[centroid[0],pole_pos[0]],'--w')\n",
    "            ax1[0].plot([centroid[1],max_pos[1]],[centroid[0],max_pos[0]],'--r')\n",
    "            ax1[0].plot([centroid[1],centroid_fluo[1]],[centroid[0],centroid_fluo[0]],'--g')\n",
    "            ax1[1].imshow(bf_straight,cmap='gray')\n",
    "            ax1[1].plot([centroid[1],pole_pos[1]],[centroid[0],pole_pos[0]],'--w')\n",
    "#            plt.show(fig1)\n",
    "#                plt.pause(10)\n",
    "            fig1.savefig(os.path.join(save_folder,'img%05d.pdf'%i))\n",
    "            plt.close(fig1)\n",
    "\n",
    "            # find angle between\n",
    "            v_morpho = (pole_pos-centroid)[::-1]\n",
    "            v_morpho[1] *= -1.\n",
    "            x_morphopole[i] = v_morpho[0]\n",
    "            y_morphopole[i] = v_morpho[1]\n",
    "            v_morpho = v_morpho/np.linalg.norm(v_morpho) # versor in the pole direction\n",
    "\n",
    "            v_fluomax = (max_pos-centroid)[::-1]\n",
    "            v_fluomax[1] *= -1.\n",
    "            x_fluomax[i] = v_fluomax[0]\n",
    "            y_fluomax[i] = v_fluomax[1]\n",
    "            v_fluomax = v_fluomax/np.linalg.norm(v_fluomax) # versor in the bra max direction\n",
    "\n",
    "            v_fluocm = (centroid_fluo-centroid)[::-1]\n",
    "            v_fluocm[1] *= -1.\n",
    "            x_fluocm[i] = v_fluocm[0]\n",
    "            y_fluocm[i] = v_fluocm[1]\n",
    "            v_fluocm = v_fluocm/np.linalg.norm(v_fluocm)\n",
    "\n",
    "            dot = np.dot(v_fluomax,v_morpho)\n",
    "            sign = np.sign((v_fluomax-v_morpho)[0])\n",
    "            alpha = sign*np.arccos(dot)\n",
    "            alphas_fluomax[i] = alpha*180/np.pi\n",
    "\n",
    "            dot = np.dot(v_fluocm,v_morpho)\n",
    "            sign = np.sign((v_fluocm-v_morpho)[0])\n",
    "            alpha = sign*np.arccos(dot)\n",
    "            alphas_fluocm[i] = alpha*180/np.pi\n",
    "\n",
    "            # find axis ratio\n",
    "            maj_ax = df_straight.major_axis_length[i]\n",
    "            min_ax = df_straight.minor_axis_length[i]\n",
    "            axratios[i] = min_ax/maj_ax\n",
    "\n",
    "        data = pd.DataFrame({\n",
    "                        'x_morphopole':x_morphopole,\n",
    "                        'y_morphopole':y_morphopole,\n",
    "                        'x_fluomax':x_fluomax,\n",
    "                        'y_fluomax':y_fluomax,\n",
    "                        'alphas_fluomax':alphas_fluomax,\n",
    "                        'x_fluocm':x_fluocm,\n",
    "                        'y_fluocm':y_fluocm,\n",
    "                        'alphas_fluocm':alphas_fluocm,\n",
    "                        'axratio':axratios,\n",
    "                        'times':times,\n",
    "                        })   \n",
    "\n",
    "        data.to_json(bra_fname)\n",
    "print('Polarity computed. Images and Bra_pole_info.json saved in result_segmentation folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
